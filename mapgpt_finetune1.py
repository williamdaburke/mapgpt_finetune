# -*- coding: utf-8 -*-
"""mapgpt_finetune1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15uUyVfvOa8Aq5YD2aYZqzmywC13Vo_3i
"""

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

!ls .

import pandas as pd
df_raw = pd.read_parquet('data.parquet', engine='pyarrow')

df_raw.head()

df_raw['language'].value_counts()

df_raw['label'].value_counts()

df_raw.shape

df_raw.to_csv('data_mapgpt.csv')

import pandas as pd
import torch
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)

# 1. Load and preprocess data
df_raw = pd.read_csv("data_mapgpt.csv")

json.loads(df_raw[1:2].chat_history.tolist()[0])

json.loads(df_raw.chat_history.head(1).tolist()[0])

for idx, row in df_raw[:5].iterrows():
  print(json.loads(row.chat_history))
  print(' [SEP] '.join(x['content'] for x in json.loads(row['chat_history'])) if row['chat_history'] else '')

# Optional: combine chat history and last user utterance
def combine_context(row):
    chat_history = ' [SEP] '.join(x['content'] for x in json.loads(row['chat_history']))
    return f"{chat_history} [SEP] {row['last_user_utterance']}"

df_raw['combined_text'] = df_raw.apply(combine_context, axis=1)

# Encode labels
label2id = {label: idx for idx, label in enumerate(df_raw['label'].unique())}
id2label = {v: k for k, v in label2id.items()}
df_raw['label_id'] = df_raw['label'].map(label2id)



# !python -m spacy download fr_core_news_sm
# !python -m spacy download es_core_news_sm
# !python -m spacy download ko_core_news_sm
# !python -m spacy download de_core_news_sm
# !python -m spacy download ja_core_news_sm

import re
import spacy

# Load spaCy models for supported languages
spacy_models = {
    'English': spacy.load('en_core_web_sm'),
    'French': spacy.load('fr_core_news_sm'),
    'Spanish': spacy.load('es_core_news_sm'),
    'Korean': spacy.load('ko_core_news_sm'),
    'German': spacy.load('de_core_news_sm'),
    'Japanese': spacy.load('ja_core_news_sm')
}

def remove_numbers(text):
    return re.sub(r'\d+', '', text)

def remove_named_entities(text, lang_name):
    nlp = spacy_models.get(lang_name)
    if not nlp:
        return text  # fallback: skip NER if no model
    doc = nlp(text)
    new_tokens = []
    for token in doc:
        if token.ent_type_:
            new_tokens.append(f"<{token.ent_type_}>")
        else:
            new_tokens.append(token.text)
    return " ".join(new_tokens)

def preprocess_multilingual(text, lang_name):
    text = remove_numbers(text)
    text = remove_named_entities(text, lang_name)
    return text



#df['clean_text'] = df.apply(lambda row: preprocess_multilingual(row['text_str'], row['language']), axis=1)

df_only_utterance_raw = df_raw.copy()
df_combined_clean = df_raw.copy()
df_only_utterance_clean = df_raw.copy()
df_raw['text_str'] = df_raw['combined_text'].astype(str)
df_only_utterance_raw['text_str'] = df_only_utterance_raw['last_user_utterance'].astype(str)
df_combined_clean['text_str'] = df_combined_clean.apply(lambda row: preprocess_multilingual(row['combined_text'], row['language']), axis=1)
df_only_utterance_clean['text_str'] = df_only_utterance_clean.apply(lambda row: preprocess_multilingual(row['last_user_utterance'], row['language']), axis=1)
df = pd.concat([df_raw, df_only_utterance_raw, df_combined_clean, df_only_utterance_clean])

df.shape

# for idx, row in df[:5].iterrows():
#   print(row.clean_text)
#   print(synonym_replacement(row.clean_text))
#   print('\n')

# Split data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text_str'], df['label_id'], test_size=0.2, stratify=df['label_id'], random_state=42
)

# 2. Tokenization
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)

# 3. Dataset class
class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),
            'labels': torch.tensor(self.labels[idx])
        }
    def __len__(self):
        return len(self.labels)

train_dataset = IntentDataset(train_encodings, list(train_labels))
val_dataset = IntentDataset(val_encodings, list(val_labels))

# 4. Model and training setup
model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-multilingual-cased',
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

training_args = TrainingArguments(
    output_dir='./results3',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=20,
    eval_strategy="epoch",
    logging_dir='./logs',
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

def compute_metrics(eval_pred):
    preds = eval_pred.predictions.argmax(axis=1)
    labels = eval_pred.label_ids
    return {
        'accuracy': accuracy_score(labels, preds),
        'f1_macro': f1_score(labels, preds, average='macro')
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# 5. Train and evaluate
trainer.train()

metrics = trainer.evaluate()
print("Final evaluation:", metrics)

!rm -r ./results/

# 6. Save model and tokenizer
model.save_pretrained("./intent_classifier_distilbert")
tokenizer.save_pretrained("./intent_classifier_distilbert")

trainer.save_model("./intent_classifier_distilbert_model")

trainer.train(resume_from_checkpoint="./intent_classifier_distilbert_model")

# Find the latest checkpoint in the output directory to resume from
import glob
import os

# Get the output directory from training_args
output_dir = training_args.output_dir

# Find all directories within the output_dir that look like checkpoints
checkpoints = sorted(glob.glob(os.path.join(output_dir, "checkpoint-*")), key=os.path.getmtime)

# Check if any checkpoints were found
if checkpoints:
    # Get the path to the latest checkpoint directory
    latest_checkpoint_path = checkpoints[-1]
    print(f"Resuming training from the latest checkpoint: {latest_checkpoint_path}")
    # Resume training from the latest checkpoint
    trainer.train(resume_from_checkpoint=latest_checkpoint_path)
else:
    print("No checkpoints found in the output directory to resume from.")
    # Optionally, you might want to handle this case, e.g., train from scratch again
    # trainer.train()

